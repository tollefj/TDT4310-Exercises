{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "620f2305-5877-4757-a192-ca477293b61c",
   "metadata": {},
   "source": [
    "# Lab 3\n",
    "We'll use this lab as an experiment of using a single file where you fill in codeblocks where necessary. They will be available as .py and .ipynb. Using the latter, or Jupyter Notebook, is highly recommended, as it provides substantially better feedback.\n",
    "\n",
    "\n",
    "Provide your outputs in a simple report, along with textual answers.\n",
    "\n",
    "\n",
    "The idea behind this format is to clarify what sort of output is required, as all answers run on tests based in the `tests.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb11d99c-3397-400d-bbcb-683c0f28a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "# feel free to import from modules of sklearn and nltk later\n",
    "# e.g., from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c800315b-4ea7-467b-8b1d-fbe945565bfe",
   "metadata": {},
   "source": [
    "## Exercise 1 - Gender detection of names\n",
    "In NLTK you'll find the corpus `corpus.names`. A set of 5000 male and 3000 female names.\n",
    "1) Select a ratio of train/test data (based on experiences from previous labs perhaps?)\n",
    "2) Build a feature extractor function\n",
    "3) Build two classifiers:\n",
    "    - Decision tree\n",
    "    - Na√Øve bayes\n",
    "    \n",
    "Finally, write code to evaluate the classifiers. Explain your results, and what do you think would change if you altered your feature extractor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fedc388-5e0d-4e9e-8470-ab141993608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenderDataset:\n",
    "    def __init__(self):\n",
    "        self.names = nltk.corpus.names\n",
    "        self.data = None\n",
    "        self.build()\n",
    "\n",
    "    def make_labels(self, gender):\n",
    "        \"\"\"\n",
    "        this function is to help you get started\n",
    "        based on the passed gender, as you can fetch from the file ids,\n",
    "        we return a tuple of (name, gender) for each name\n",
    "        \n",
    "        use this in `build` below, or do your own thing completely :)\n",
    "        \"\"\"\n",
    "        return [(n, gender) for n in self.names.words(gender + \".txt\")]\n",
    "    \n",
    "    def build(self):\n",
    "        \"\"\" TODO\n",
    "        combine the data in \"male\" and \"female\" into one\n",
    "        remember to randomize the order\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def split(self, ratio):\n",
    "        return train_test_split(self.data, test_size=ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f99b277-58e2-41f4-b5cf-5cd7f1e1a1db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, classifier):\n",
    "        self.classifier = classifier\n",
    "        self.model = None\n",
    "    \n",
    "    def train(self, data):\n",
    "        # TODO: train classifier and store model\n",
    "        pass\n",
    "        \n",
    "    def test(self, data):\n",
    "        # TODO: return accuracy for the model on input data\n",
    "        pass\n",
    "    \n",
    "    def train_and_evaluate(self, train, test):\n",
    "        self.train(train)\n",
    "        return self.test(test)\n",
    "        \n",
    "    def show_features(self):\n",
    "        # OPTIONAL\n",
    "        pass\n",
    "\n",
    "                                 \n",
    "class FeatureExtractor:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.features = []  \n",
    "        \n",
    "        self.build()\n",
    "                 \n",
    "    @staticmethod\n",
    "    def text_to_features(name):\n",
    "        # TODO: create a dict of features from a name\n",
    "        return {\n",
    "            \"name\": name\n",
    "        }\n",
    "    \n",
    "    def build(self):\n",
    "        # TODO: populate your features with the above function\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ae27ee-b7af-4961-87c9-1082174243e3",
   "metadata": {},
   "source": [
    "Note: you should achieve an accuracy of well above 70%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd040ab-42c4-42ed-ad8c-b5fbf4bfa718",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.01  # TODO: modify\n",
    "train, test = GenderDataset().split(ratio=split_ratio)\n",
    "\n",
    "classifiers = {\n",
    "    \"decision_tree\": Classifier(None), # TODO\n",
    "    \"naive_bayes\": Classifier(None), # TODO\n",
    "}\n",
    "\n",
    "train_set = FeatureExtractor(train).features\n",
    "test_set = FeatureExtractor(test).features\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    acc = classifier.train_and_evaluate(train_set, test_set)\n",
    "    print(\"Model: {}\\tAccuracy: {}\".format(name, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9acdd9d-789d-4d05-bad4-35141ffa5e4b",
   "metadata": {},
   "source": [
    "## Exercise 2 - Spam or ham\n",
    "Spam or ham is referred to a mail being spam or regular (\"ham\"). Follow the instructions and implement the `TODOs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb643e2-3d3c-487f-a953-092fb334c27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = pd.read_csv(\n",
    "    'spam.csv',\n",
    "    usecols=[\"v1\", \"v2\"],\n",
    "    encoding=\"latin-1\"\n",
    ").rename(columns={\"v1\": \"label\", \"v2\": \"text\"})\n",
    "\n",
    "print(spam.label.value_counts())\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e663bc-07ce-4593-8298-350afaee23fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO: transform label to numerical\n",
    "Expected output:\n",
    "0    4825\n",
    "1     747\n",
    "Name: label, dtype: int64\n",
    "\n",
    "hint: you can use \"apply\" or \"replace\" for a column in pandas\n",
    "\"\"\"\n",
    "spam.label = spam.label # your transformation goes here\n",
    "spam.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027995b9-aee1-4d67-8fa1-f6cd3afe1795",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner:\n",
    "    def __init__(self, text):\n",
    "        self.text = [] # TODO: tokenize\n",
    "        self.stemmer = None # TODO: incorporate a stemmer of your choice\n",
    "        self.stopwords = None  # TODO: you've done this a few times\n",
    "        self.lem = None  # TODO: lemmatizer\n",
    "    \n",
    "    \"\"\"\n",
    "    Create small functions to replace your tokens (self.text)\n",
    "    iteratively. Such as a lowercase function.\n",
    "    \"\"\"\n",
    "    def lowercase(self):\n",
    "        self.text = [w.lower() for w in self.text]\n",
    "\n",
    "    def clean(self):\n",
    "        self.lowercase()\n",
    "        \"\"\"\n",
    "        TODO: populate with your defined cleaning functions here\n",
    "        perhaps you want some conditional values to\n",
    "        control which functions to use?\n",
    "        \"\"\"\n",
    "        \n",
    "        # finally, return it as a text \n",
    "        return \" \".join(self.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9cac1b-141f-43c6-939b-beddea37ade9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean = lambda text: TextCleaner(text).clean()\n",
    "spam.text = spam.text.apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52288899-68e3-4d84-bd02-609419b14673",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b2bad2-d747-49fe-8876-d8bf19e10095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "split_ratio = 0.01 # TODO: modify\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    spam.text, spam.label, test_size=split_ratio, random_state=4310)\n",
    "\n",
    "# TODO: vectorize with sklearn\n",
    "vectorizer = None\n",
    "# TODO: fit the vectorizer to your training data\n",
    "X_train = None\n",
    "\n",
    "# TODO: set up a multinomial classifier\n",
    "classifier = None\n",
    "if classifier:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "vectorized = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f9bdb3-b4b4-40ba-9faa-d7304c741401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, vectorizer, data, all_predictions=False):\n",
    "    data = None # TODO apply the transformation from the vectorizer to test data \n",
    "    if all_predictions:\n",
    "        return model.predict_proba(data)\n",
    "    else:\n",
    "        return model.predict(data)\n",
    "\n",
    "def print_examples(data, probs, label1, label2, n=10):\n",
    "    percent = lambda x: \"{}%\".format(round(x*100, 1))\n",
    "\n",
    "    for text, pred in list(zip(data, probs))[:n]:\n",
    "        print(\"{}\\n{}: {} / {}: {}\\n{}\".format(\n",
    "            text,\n",
    "            label1,\n",
    "            percent(pred[0]),\n",
    "            label2,\n",
    "            percent(pred[1]),\n",
    "            \"-\" * 100  # to print a line\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1329d63-5ed4-4648-adca-ab36567095a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if classifier:\n",
    "    y_probas = predict(classifier, vectorizer, X_test, all_predictions=True)\n",
    "    print_examples(X_test, y_probas, \"ham\", \"spam\", n)\n",
    "\n",
    "    y_pred = predict(classifier, vectorizer, X_test)\n",
    "    # TODO display a confusion matrix on the test set vs predictions\n",
    "    confusion_mat = None\n",
    "    print(confusion_mat)\n",
    "\n",
    "    # show precision and recall in a confusion matrix\n",
    "    tn, fp, fn, tp = confusion_mat.ravel()\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "\n",
    "    print(\"Recall={}\\nPrecision={}\".format(round(recall, 2), round(precision, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a39c5-f75f-4298-ac1d-48d11604a063",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 3 - Word features\n",
    "Word features can be very useful for performing document classification, since the words that appear in a document give a strong indication of what its semantic content is. However, many words occur very infrequently, and some of the most informative words in a document may never have occurred in our training data. One solution is to make use of a lexicon, which describes how different words relate to each other.\n",
    "\n",
    "Your task:\n",
    "- Use the WordNet lexicon and augment the movie review document classifier (See NLTK book, Ch. 6, section 1.3) to use features that generalize the words that appear in a document, making it more likely that they will match words found in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea34bc3-b944-4850-9f20-104898a6cbc2",
   "metadata": {},
   "source": [
    "Download wordnet and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d70ac-637e-442d-abf0-d9a81a3917fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import wordnet as wn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88196d95-5ebc-4f64-b872-5f68cb1634b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement \n",
    "def word_to_syn(word) -> list:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d9d4a7-c64f-4509-87c7-77fb3ed2fa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this is from Ch. 6, sec. 1.3, with slight modifications\n",
    "note that word_to_syn(word) (from the above implementation)\n",
    "is in the beginning of the following function\n",
    "\"\"\"\n",
    "documents = [([word_to_syn(word) for word in list(movie_reviews.words(fileid))], category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "n_most_freq = 2000\n",
    "word_features = list(all_words)[:n_most_freq]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f2b8d0-d8f4-44f6-b61b-c7ab867ffab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d, c) in documents]\n",
    "\n",
    "split_ratio = 0.01 # TODO: modify\n",
    "train_set, test_set = train_test_split(featuresets, test_size=split_ratio)\n",
    "\n",
    "# TODO: select a suitable classifier\n",
    "classifier = None\n",
    "model = classifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc056fa9-9cbb-492d-90b3-75014a89e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: return a flattened list of input words and their lemmas\n",
    "def synset_expansion(words) -> list:\n",
    "    pass\n",
    "\n",
    "expanded_word_features = synset_expansion(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b87242-0f80-49d0-b9d2-d9de9a647938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some assertions to test your code :-)\n",
    "assert sorted(synset_expansion([\"pc\"])) == [\"microcomputer\", \"pc\", \"personal_computer\"]\n",
    "assert sorted(synset_expansion([\"programming\", \"coder\"])) == [\n",
    "    'coder',\n",
    "    'computer_programing',\n",
    "    'computer_programmer',\n",
    "    'computer_programming',\n",
    "    'program',\n",
    "    'programing',\n",
    "    'programme',\n",
    "    'programmer',\n",
    "    'programming',\n",
    "    'scheduling',\n",
    "    'software_engineer'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac2370a-6e85-4b3e-a9e4-5f4dbeab15f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_featuresets = [(document_features(d), c) for (d, c) in documents]\n",
    "doc_train_set, doc_test_set = train_test_split(doc_featuresets, test_size=0.1)\n",
    "\n",
    "doc_model = model.train(doc_train_set)\n",
    "doc_model.show_most_informative_features(5)\n",
    "print(\"Accuracy: \", nltk.classify.accuracy(doc_model, doc_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ec224-e865-47ee-976a-d72d5184a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexicon_features(reviews):\n",
    "    review_words = set(reviews)\n",
    "    features = {}\n",
    "    for word in expanded_word_features:\n",
    "        if word not in word_features:\n",
    "            features['synset({})'.format(word)] = (word in review_words)\n",
    "        features['contains({})'.format(word)] = (word in review_words)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e280bf-409d-4444-9d70-bf95a8ba2d34",
   "metadata": {},
   "source": [
    "Question: do you see any issues with including the synsets? Experiment a bit with different words and verify your ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a88411-251a-4acd-b83c-ed287c17c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning: this may take some time to run\n",
    "lex_featuresets = [(lexicon_features(d), c) for (d, c) in documents]\n",
    "lex_train_set, lex_test_set = train_test_split(lex_featuresets, test_size=0.1)\n",
    "lex_model = model.train(lex_train_set)  # the same classifier as you defined above\n",
    "lex_model.show_most_informative_features()\n",
    "print(\"Accuracy: \", nltk.classify.accuracy(lex_model, lex_test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53b328-bbcb-4895-9047-ebaa3e9b543c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 4 -- Experimentation\n",
    "This exercise is largely open to experiment with and testing your skills thus far!\n",
    "Large websites are an ideal place to look for large corpora of natural language. In this exercise, you're free to implement what you've learned on real-world data, mined from youtube (see `youtube_data`). Reuse classes defined earlier on in the exercise if you want.\n",
    "\n",
    "The only requirement here is to **use a classifier not previously used in the exercise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17afb16-42bb-4df2-a817-9dcc1264d75e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('base')",
   "language": "python",
   "name": "python399jvsc74a57bd029553384b1b01f6109f5069a08d409f2dc5adeb046ccd0e94d694cc3c1cd07a6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
