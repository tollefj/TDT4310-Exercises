{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b611103-9578-478d-86ba-446f4a2bdd08",
   "metadata": {},
   "source": [
    "# Practice stuff\n",
    "This is taken from NLTK chapter 7, with slight modifications :) Note that this overlaps quite a bit with the next lab on information extraction - so it pays off to read through this chapter and experiment a bit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e54d0-3297-41d8-bcd6-783090a99bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0017f56-7601-4544-86c1-7461c97e13f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a helper function for a given chunker, that tokenizes -> pos tags\n",
    "def parse_sent(parser, sent):\n",
    "    return parser.parse(nltk.pos_tag(nltk.word_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b275162-1d21-492e-839f-09da2b915521",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"the little bear saw the fine fat trout in the brook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c442f9e7-e140-4fb6-b5a3-e3f56d1d5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "NP: {<DT><JJ><NN>} # DT (optional adjective) followed by NN\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "parse_sent(cp, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f0ff2b-a0d4-4bf8-a889-1b7925c21c45",
   "metadata": {},
   "source": [
    "This isn't quite right. \"The fine fat\" is now a NP, not \"the fine fat trout\".. We must allow multiple nouns for NPs (add a +)!\n",
    "\n",
    "Also, let's incorporate optional adjectives, such that \"the brook\" becomes a NP (? after the definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e91fe57-88fa-4367-87c3-40961f1ddc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "NP: {<DT><JJ>?<NN>+} # DT (optional adjective) followed by NN\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "parse_sent(cp, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d14a81-8ca4-4072-9ea3-a28531d6cc4c",
   "metadata": {},
   "source": [
    "Fixed! Now we need to support VPs as we saw in the lecture. VP = V NP or PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0168d1d-7ab7-4c89-988a-c983affb5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "NP: {<DT><JJ>?<NN.*>+} # DT (optional adjective) followed by NN\n",
    "PP: {<IN><NP>}         # prepositions followed by NP\n",
    "VP: {<VB.*><NP|PP>+}   # match one or more of NP or PP\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "parse_sent(cp, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a84d4-0107-4104-bb5b-6bb27d04c117",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"the girl that worked at the university ntnu\"\n",
    "parse_sent(cp, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4c8290-644c-4698-8f12-c7ad3286a6c1",
   "metadata": {},
   "source": [
    "## Explore subtrees!\n",
    "Use the brown corpus, iterate a few sentences and parse them with the grammar:\n",
    "\n",
    "any verb &rarr; TO &rarr; any verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7e24d3-d6bd-43ef-a23d-5c3497fcc8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser('chonker: {<V.*> <TO> <V.*>}')\n",
    "brown = nltk.corpus.brown\n",
    "for sent in brown.tagged_sents()[:200]:  # <-- limit to avoid print spam\n",
    "    tree = cp.parse(sent)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'chonker':\n",
    "            print(\" \".join([w for w, POS in subtree.leaves()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a932b-d0ae-43f1-b56c-4459f74ee314",
   "metadata": {},
   "source": [
    "## Reverse the operation with Chinks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170945e-4a7c-4bb6-b010-f3fbf20166ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP:\n",
    "    {<.*>+}          # Chunk everything\n",
    "    }<V.*|IN>+{      # Chink sequences of V and IN\n",
    "  \"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = \"the little bear saw the fine fat trout in the brook\"\n",
    "parse_sent(cp, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3efb64c-279c-4064-b25c-a4160b8602d2",
   "metadata": {},
   "source": [
    "Observe how the `DT JJ JJ NN` sequence is now a NN because of the VBD|IN chink!\n",
    "\n",
    "We also chunked \"the brook\" correctly. Amazing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f76f783-d7cd-40f9-83ca-971caac725d3",
   "metadata": {},
   "source": [
    "# Evaluation with predfined chunked sentences\n",
    "Using annotated data sets, you can create your own chunker and evaluate it on true sentences :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6731ee47-a777-4bf5-8890-af0a20b697ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsj = nltk.corpus.conll2000\n",
    "wsj.chunked_sents(\"train.txt\")[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf79081-d130-4bc1-a803-8076dfa3c37d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## specify chunk types\n",
    "only chunks on NPs. Note how the VP above is now not chunked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02447045-f165-44dc-815e-457a6d71a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsj.chunked_sents(\"train.txt\", chunk_types=[\"NP\"])[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b95e77-ddd5-4cfc-b582-aaaab00aef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(\"\")  # empty regex parser\n",
    "train_sents = wsj.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "test_sents = wsj.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "print(cp.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a66d49-69d6-4218-a7ee-eb77568bd07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"NP: {<[CDJNP].*>+}\"  # populate some rules\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa34c165-c4bc-4331-8133-dcbdf40c5e15",
   "metadata": {},
   "source": [
    "## Improve with a custom chunker!\n",
    "use the training corpus to find the chunk tag (I, O, or B) that is most likely for each part-of-speech tag. In other words, we can build a chunker using a unigram tagger. But rather than trying to determine the correct part-of-speech tag for each word, we are trying to determine the correct chunk tag, given each word's part-of-speech tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0327659c-ce8e-49ed-8f46-d276133fe689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents, tagger=nltk.UnigramTagger):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = tagger(train_data)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d537d5-52dc-4809-9a8f-be6b8f0a8460",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_chunker = NgramChunker(\n",
    "    train_sents,\n",
    "    tagger=nltk.BigramTagger\n",
    "    # can be unigram, trigram, or whatever you implemented in earlier labs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f84c1e-ab4e-494a-985d-133dfddc8606",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ngram_chunker.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f261b025-0c97-4e27-9a7c-1a7d74d341cf",
   "metadata": {},
   "source": [
    "## Recursion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07fe5ae-da14-4859-8917-dd051a9628db",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
    "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
    "  CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
    "  \"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "txt = \"Mary saw the cat sit on the mat\"\n",
    "parse_sent(cp, txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ecb3f8-13b1-4702-b143-542c3e2e2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"John thinks Mary saw the cat sit on the mat\"\n",
    "parse_sent(cp, txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a379d296-5b1d-4a29-becf-1f8a36084d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(grammar, loop=2)\n",
    "parse_sent(cp, txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4375ee-577a-4ea0-afa9-93f27291b2ea",
   "metadata": {},
   "source": [
    "# tree traversal\n",
    "a generic traverse algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518c90c6-d08f-4efb-8844-cbda843d798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(t):\n",
    "    try:\n",
    "        t.label()\n",
    "    except AttributeError:\n",
    "        print(t, end=\" \")\n",
    "    else:\n",
    "        # Now we know that t.node is defined\n",
    "        print('(', t.label(), end=\" \")\n",
    "        for child in t:\n",
    "            traverse(child)\n",
    "        print(')', end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23357f-c064-4e7e-aca5-e62492ac509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = parse_sent(cp, txt)\n",
    "traverse(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b758ba8-2d2d-45ec-b2a4-496c349d41fb",
   "metadata": {},
   "source": [
    "# named entity recognition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133d93dd-8b40-4885-8293-290eeb3e1e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = nltk.corpus.treebank.tagged_sents()[1]\n",
    "print(\" \".join([w for w, pos in sent]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6862c9-8415-404e-af5f-8456f464f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.ne_chunk(sent, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c90e81-6806-4aed-8fc0-8f09a1a9b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.ne_chunk(sent)  # note how this separates singular entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0eb542-e69c-49f2-8ed9-dc954946be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern=pattern):\n",
    "        print(nltk.sem.rtuple(rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e73ca7-ab37-40b6-a5a3-c45c9282b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2002\n",
    "vnv = \"\"\"\n",
    "(\n",
    "is/V|    # 3rd sing present and\n",
    "was/V|   # past forms of the verb zijn ('be')\n",
    "werd/V|  # and also present\n",
    "wordt/V  # past of worden ('become)\n",
    ")\n",
    ".*       # followed by anything\n",
    "van/Prep # followed by van ('of')\n",
    "\"\"\"\n",
    "VAN = re.compile(vnv, re.VERBOSE)\n",
    "for doc in conll2002.chunked_sents('ned.train'):\n",
    "    for rel in nltk.sem.extract_rels('PER', 'ORG', doc,\n",
    "                                     corpus='conll2002', pattern=VAN):\n",
    "        print(nltk.sem.clause(rel, relsym=\"VAN\"))\n",
    "        print(nltk.rtuple(rel, lcon=True, rcon=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c4f18-2317-4f74-bd57-07a1179817a9",
   "metadata": {},
   "source": [
    "# Making use of treebanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e4acb8-7253-4498-a98f-2ee6ef869812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b773a69b-641d-4ece-8c47-e4f95c6c777b",
   "metadata": {},
   "source": [
    "Create a production rule of the form VP &rarr; S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf26c0-d0bc-4dfc-83c1-4745d40536a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vp_s(tree):\n",
    "    child_nodes = [child.label() for child in tree\n",
    "                   if isinstance(child, nltk.Tree)]\n",
    "    return  (tree.label() == 'VP') and ('S' in child_nodes)\n",
    "\n",
    "def np_pp(tree):\n",
    "    child_nodes = [child.label() for child in tree\n",
    "                   if isinstance(child, nltk.Tree)]\n",
    "    return  (tree.label() == 'NP') and ('PP' in child_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe70223-e5a9-447e-91c7-ef1206b9ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = treebank.parsed_sents()[:10]  # sample from first 10\n",
    "\n",
    "def np_pp(tree):\n",
    "    child_nodes = [child.label() for child in tree\n",
    "                   if isinstance(child, nltk.Tree)]\n",
    "    return  (tree.label() == 'NP') and ('PP' in child_nodes)\n",
    "for tree in sents:\n",
    "    for subtree in tree.subtrees(vp_s):\n",
    "        print(subtree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6bf746-538d-44db-8e5c-35ef28ff0d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tree in sents:\n",
    "    for subtree in tree.subtrees(np_pp):\n",
    "        print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e4401e-673c-4e6b-a8d5-3a2464cba742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5692df-2c3b-466b-a67a-98d4921220cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python397jvsc74a57bd029553384b1b01f6109f5069a08d409f2dc5adeb046ccd0e94d694cc3c1cd07a6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
